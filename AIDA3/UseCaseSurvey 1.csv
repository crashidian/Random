Timestamp,Email Address,Your first name,Your last name,Problem areas: Please specify the problem area related to AAVs that your use case addresses. Please select the most applicable one! ,Perception & Navigation,Environment,Counterintelligence & Cybersecurity,Urban Airspace Management,UAV Control,UAV Operations & HAT,Other,Active SLAM,Artificial General Intelligence (AGL),Computer vision,Digital twins and fidelity simulations,Edge computing,Embodied AI,Explainability and interpretability of AI/ML,Federated learning,Formal methods,Generative AI for data augmentation,Large Language Models (LLMs),Model-based control,Physics inspired neural networks (PINNs),Reinforcement learning,Safety in non-linear control theory (e.g. control barrier functions),Other,Title of your submission: Please provide a short title of your research use case,"Your pitch: Please provide a short description of research use case idea! 

Please address the following:
1) What is the research question that you would to answer? 
2) What components of the AIrTonomy infrastructure would you like to use? Please review the material here, and also watch the video and the recording of our webinar. 
3) How would you use those components to design and validate AI/ML models, architectures and systems? 
4) How would this research create new discoveries and make broader impacts? 
","Extended abstract: We would like to learn more! Please upload your extended abstract here to provide further details (PDF maximum 4 pages). 
Please include: 
1) Title
2) Author details
3) Keywords
4) Short abstract
5) Introduction and need problem statement
6) The method/approach describing the use of the AIrTonomy infrastructure components and the Purdue Unmanned aerial Proving Ground (PUP). 
7) The unique needs in terms of physical infrastructure (e.g. urban canyons, motion-capture systems, vehicles, onboard sensors, onboard computing hardware, indoor, outdoor, human-brain-computer interfaces, wearable devices, ground communication infrastructure (e.g. radar, sensors, 4/5G etc.), physical twins)
8) The unique needs in terms of cyber-infrastructure (real-time streaming, digital twins, datasets, cloud resources for ML training, etc.)

Tables, figures, images are encouraged! Find the template to submit your uses cases here - Slidedeck or Document ",PURT,"How would you like to use PURT remotely? Do you have specific requirements concerning such an UAS indoor motion capture lab? Do you have specific requirements with respect to emulating certain communication and networking conditions or other real-world environmental conditions (e.g. GNSS signal degradation, wind, urban canyons, etc.)? ",AAL,"How would you like to use the SOC? (e.g. Do you have any specific requirements in terms of sensors? Do you have any specific requirements in terms of VR/AR technologies? How large is the team size you want to work with? Do you have specific requirements in terms of sensors to perform research using real-time sensing of human cognition (e.g. eyetracking etc.) and brain-computer interfaces and neuro-inspired human-robot-interaction (e.g. biofeedback, neurofeedback)? What requirements do you have with respect to our motion-capture systems and cameras?)",PUC,"How would you like to use the PUC? What are your specific requirements in terms of motion-capture, urban infrastructures (e.g. materials, etc.)? What forms of signal degradation do you want to study and how? ",Earhart Field,"How would you like to use the PUP Airfield? Are there specific requirements in onsite equipment and sensors? Besides collecting ground truth data, what are data matter? ","Our fleet and onboard sensors/processing capacity: Researchers both at Purdue and in other research institutes will have access to our fleet of UAVs, from larger fixed-wing aircraft, VTOL aircraft, as well as smaller multi-rotor platforms. At Purdue, we own and operate two midscale UAVs with a wingspan of 31 ft, Maximum Takeoff Mass of 1,000+ lbs, and a range of 600+ miles. As large fixed-wing drones, they have takeoff and landing requirements more closely aligned to traditional manned aviation, and will operate from the PUP Main Runway.
These aircraft are designed around a flexible payload bay, allowing researchers up to 300 lbs & 185 gal of capacity to install and test sensing payloads. This exceptional payload ability unblocks several limitations that existing UAVs pose, and can carry sensors that previously only manned aircraft could accommodate. How important is the access the different vehicles?  [Large fixed-wing vehicles]","Our fleet and onboard sensors/processing capacity: Researchers both at Purdue and in other research institutes will have access to our fleet of UAVs, from larger fixed-wing aircraft, VTOL aircraft, as well as smaller multi-rotor platforms. At Purdue, we own and operate two midscale UAVs with a wingspan of 31 ft, Maximum Takeoff Mass of 1,000+ lbs, and a range of 600+ miles. As large fixed-wing drones, they have takeoff and landing requirements more closely aligned to traditional manned aviation, and will operate from the PUP Main Runway.
These aircraft are designed around a flexible payload bay, allowing researchers up to 300 lbs & 185 gal of capacity to install and test sensing payloads. This exceptional payload ability unblocks several limitations that existing UAVs pose, and can carry sensors that previously only manned aircraft could accommodate. How important is the access the different vehicles?  [VTOL aircraft]","Our fleet and onboard sensors/processing capacity: Researchers both at Purdue and in other research institutes will have access to our fleet of UAVs, from larger fixed-wing aircraft, VTOL aircraft, as well as smaller multi-rotor platforms. At Purdue, we own and operate two midscale UAVs with a wingspan of 31 ft, Maximum Takeoff Mass of 1,000+ lbs, and a range of 600+ miles. As large fixed-wing drones, they have takeoff and landing requirements more closely aligned to traditional manned aviation, and will operate from the PUP Main Runway.
These aircraft are designed around a flexible payload bay, allowing researchers up to 300 lbs & 185 gal of capacity to install and test sensing payloads. This exceptional payload ability unblocks several limitations that existing UAVs pose, and can carry sensors that previously only manned aircraft could accommodate. How important is the access the different vehicles?  [small multi-rotor aircrafts]","Our fleet and onboard sensors/processing capacity: Researchers both at Purdue and in other research institutes will have access to our fleet of UAVs, from larger fixed-wing aircraft, VTOL aircraft, as well as smaller multi-rotor platforms. At Purdue, we own and operate two midscale UAVs with a wingspan of 31 ft, Maximum Takeoff Mass of 1,000+ lbs, and a range of 600+ miles. As large fixed-wing drones, they have takeoff and landing requirements more closely aligned to traditional manned aviation, and will operate from the PUP Main Runway.
These aircraft are designed around a flexible payload bay, allowing researchers up to 300 lbs & 185 gal of capacity to install and test sensing payloads. This exceptional payload ability unblocks several limitations that existing UAVs pose, and can carry sensors that previously only manned aircraft could accommodate. How important is the access the different vehicles?  [Customizable onboard sensing stack]","How would you like to use our fleet? (e.g. What vehicles would you like to use and how? Do you have specific requirements in terms of onboard sensing (e.g. SAR, airborne radars, LiDAR, thermal and hyperspectral, etc) or onboard computing resources? What vehicles would you like to study and how? Do you want to bring your own vehicles?) ","Purdue XTM: To ensure integration into real urban airspaces, and allow vehicles to fly within PUP, we will work closely with our partner SAAB to install ground communication and networking infrastructure for urban airspace management.  What we call the Purdue XTM system will produce air traffic and airspace data of high value to our research community.  One key element of Purdue XTM is an active R1410 Xband radar. Combined with three passive radars placed at the outside of the PUP, we can identify small UAVs flying at low altitude within the airspace. In addition to these radars, we will also establish a Low Altitude Weather Network, a distributed and meshed network of low-cost weather sensors that provides accurate low-altitude aviation meteorological data not delivered by other sources, and remote-ID solutions that support the new FAA regulatory requirements for AAVs.  In addition, we will also equip part of our locations with 4 and 5G wireless network infrastructure. How important is the Purdue XTM system for your research?  ",How would you like to use Purdue XTM? (e.g. are their specific radar requirements? Is there a need for portable radars? What are particular requirements with respect to low altitude weather network station? Where do you want to place them?) ,Digital Twins,Remote Operations,Onboard Software,AI/ML Workspace,Data and Research Services,AI Experimentation,"Digital twins: Our digital twin engine will rely on a modular architecture. We will implement the digital twin engine with a focus on six modules and features: (1) The twin engine will include a set of standard digital models, including asset models of the facilities (PUC and PURT), airspace models, standard vehicle models (equipped with a set of sensors), as well as human
operator models. Users can upload their data from simulations performed in their own simulation environments, or record simulation data gathered from simulations in our cloud-based simulation
environment. Further, they can also initiate new experiments or port their updated models onto onboard software.The engine will include a toolkit that allows our community of digital twin researchers to easily extend our digital models, and to create twins for a particular physical system of their interest. How important are such digital twins for your research? ",How would you like to use a digital twin? Do have a specific physical system in mind for which you need high fidelity simulation models? How would you like to use such digital twins (e.g. simulations and counterfactual reasoning etc.),RSVP: Please do not forget to RSVP for the event now. I confirm that I would like to physically attend the AIrTonomy 2-day workshop on Sept. 11-12.  ,Do you have colleagues that should be invited to AIrTonomy?  Please provide their emails below. ,Please leave any questions or comments below.  Thank you!,"Please submit your poster here, you can find the template here"
9/8/24 23:28,bperseghetti@rudislabs.com,Benjamin,Perseghetti,AAV Operations,1,1,1,1,1,1,,1,1,1,1,1,,1,1,1,1,,1,1,1,1,,NXP Mobile Robotics,"The NXP mobile robotics team is interested in providing robust and scale-able robotics developments platforms. NXP has been helping to develop CogniPilot along with Purdue and is interested in testing the robustness of this software and related hardware platforms in real-world settings. We envision leveraging PURT for rapid development of mobile robotics solutions with motion-capture provided ground truth. In addition, the Purdue Urban Canyon lab would be very advantageous for urban robotics testing with outdoor ground truth. NXP has not yet released a fixed-wing development platform for CogniPilot, but testing could be conducted at PURT and later deployed at the PUP airfield. We believe the SOC may also prove beneficial for studying human operator interaction with mobile robotics. AI/ML are critical tools for robotics and  NXP hardware leveraged for CogniPilot is specifically designed to enable neural net based solutions. Testing at PUP as part of AIDA3 will help ensure that develop products leveraging AI/ML are robust. We also envision that the facility may serve as a location for future mobile robotics competitions and cutting edge research and development for the future of AI/ML in mobile robots.",https://drive.google.com/open?id=1KR_Oyx59JjpkaEKMeigHqXWDIM0aIArh,5,We would like to use PURT in person.,4,We would like to study human robot teaming with mobile robotic platforms.,5,We would like to study GPS signal degradation for MR,4,We would like to test future fixed wing MR platforms,Low,High,High,High,"To test CogniPilot or other open, secure, and safe by design Autopilots. We would like to bring our vehicles as well.",3,Not applicable to current needs but might be in future.,High,Low,High,High,Low,Low,High,We would like to test the accuracy of our existing digital twins for mobile robotics platforms.,"No, I am not able to attend in person but I would like to be considered for the best contribution award.",,,
9/8/24 21:03,dyeke@purdue.edu,Doguhan ,Yeke,AAV Operations,1,,1,,1,,,1,1,,,,,,,1,,,,,,1,,Adaptive Sensor Fusion for Degraded Environments in AAV Networks,"Traditional sensor fusion approaches often rely heavily on onboard sensor data, which can degrade in challenging environments, such as urban areas Autonomous operations for uncrewed aerial vehicles (UAVs) serve a larger interest in the aerospace
industry with the demand to further expand its capabilities to provide not only efficient but also safe
with GNSS interference or high-altitude operations affecting rangefinders. Our approach innovatively addresses these challenges by integrating
external information from other AAVs within the network that have accurate sensor data. This collaborative fusion helps mitigate sensor degradation and enhance overall network performance.",,5,GNSS signal degradation,5,VR/AR would be interesting to use.,5,Motion-capture and spoofing scenarios would be great.,5,,,,High,,,5,,High,High,High,High,High,High,High,,"Yes, I would like to attend in person.",,,https://drive.google.com/open?id=1fiVgOgXXdFsoYilh2hyVPox_xySMZRpV
9/8/24 23:59,dyeke@purdue.edu,Doguhan,Yeke,AAV Operations,1,,1,,1,,,1,1,,,,1,,,,,,,,,,,Adaptive Sensor Fusion for Degraded Environments in AAV Networks,"Traditional sensor fusion approaches often rely heavily on onboard sensor data, which can degrade in challenging environments, such as urban areas with GNSS interference or high-altitude operations affecting rangefinders. Our approach innovatively addresses these challenges by integrating external information from other AAVs within the network that have accurate sensor data. This collaborative fusion helps mitigate sensor degradation and enhance overall network performance.
Degraded GNSS Scenarios: In large cities, GNSS signals degrade due to signal reflections and obstructions. AAVs in unaffected zones can share their accurate positions with those in degraded zones, allowing the affected AAV to compensate for the GNSS inaccuracy by relying on relative positioning and its onboard sensors.
Rangefinder Limitations: When rangefinders lose accuracy at higher altitudes, other AAVs closer to the ground or with unaffected rangefinders can provide relative distance data, helping to maintain environmental awareness.
Novel Approach: Unlike traditional sensor fusion, which focuses on integrating only onboard sensors, this project proposes using external data from nearby AAVs. By sharing accurate sensor data across the network, the system adapts to sensor-degraded environments and enhances the decision-making capabilities of affected AAVs. PUP can provide the capabilities for this as we will be able to operate in GNSS-degraded environment.",,5,,5,,5,,5,,,,High,High,,5,,High,High,High,High,,,High,,"Yes, I would like to attend in person.",,,https://drive.google.com/open?id=17B1Vq8otUzKJPreXemR2FILN1tUQOFu6
8/28/24 23:23,li213@illinois.edu,Yangge,Li,AAV Operations,1,,,,,,,,,,,,,,,1,,,1,,,1,,Agile Vision-Based Tracking,"Inspired by developments in UAVs, We are interested in exploring the topic of vision-based target tracking for multi-rotor UAVs. Specifically, we want to solve the problem of a chaser drone tracking a leader drone that follows complex trajectories using only vision feedback. The drone tracking problem we are solving is central to critical applications, such as in-air refueling, space craft docking, and aerial surveillance. The vision-based algorithm can also provide navigation of the drone under GPS denied environment. The complexity of this problem arises not only from the need to accurately identify and follow the target using a vision pipeline but also from the necessity to maintain robustness despite environmental variations and the dynamic movements of the target. The chaser drone must be capable of responding effectively when visual contact with the target is temporarily lost.  

The whole research can be divided into several steps. The first step is to design an end-to-end autonomy pipeline for performing this drone tracking task. We propose a control strategy that utilizes trajectory predictions generated using reachability analysis [1] for the chaser drone when visual information about the leader drone is unavailable. 

The second step involves finding some performance guarantee on the tracking algorithm and evaluating the algorithm in both simulated environment and actual hardware. We are hoping to provide some theoretical guarantee about the tracking performance of the chaser, even considering that we may temporarily lose vision. We want to see when the leader drone is following under different types of trajectories, the chaser drone is tracking the leader drone at different distances and under different performance of the chaser drone, what will be the performance of our tracking algorithm. 

Furthermore, we are interested in the impact of environmental variation on the vision pipeline and how that impacts our overall system. We can utilize idea such as perception contract [2,3] to study the behavior of the perception pipeline under various environmental conditions and the behavior of the overall tracking system. We can potentially find the operational design domain of the system for a given combination of environment variations. 

[1] Verse: A Python library for reasoning about multi-agent hybrid system scenarios arxiv Yangge Li, Haoqing Zhu, Katherine Braught, Keyi Shen, Sayan Mitra. In the proceedings of Computer Aided Verification (CAV), LNCS vol 13964, pages 351–364, 2023.

[2] C. Hsieh, Y. Li, D. Sun, K. Joshi, S. Misailovic and S. Mitra, ""Verifying Controllers With Vision-Based Perception Using Safe Approximate Abstractions,"" in IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, vol. 41, no. 11, pp. 4205-4216, Nov. 2022.

[3] Li, Yangge, Benjamin C Yang, Yixuan Jia, Daniel Zhuang and Sayan Mitra. “Refining Perception Contracts: Case Studies in Vision-based Safe Auto-landing.” ArXiv abs/2311.08652 (2023).",,5,,2,,4,,3,,High,High,High,High,,2,,Low,High,High,Low,Low,Low,Low,,"No, I am not able to attend in person but I would like to be considered for the best contribution award.",,,
8/31/24 10:30,subhashish.chakravarty@collins.com,Subhashish,Chakravarty,AAV Operations,1,,1,,1,1,,1,1,1,1,1,1,1,1,,,1,1,1,1,,1,50 shades of counter-autonomy,"1) Given some (with a stretch goal of 50) AI Algorithms aiding autonomy in a mission with swarms, can we figure out (provide guarantees) which algorithm did which task? 2) We would need multiple missions, aavs, instrumentation data, flight range, digital twins of aavs etc. The other required Airtonomy components will be decided during problem scoping.3) The research techniques/architectures/systems would be validated against similar architectures used in the DoD. 4) This research is geared towards the burgeoning counter-autonomy thrust of the DoD and would enable future products capable of delivering strategic surprise.",,5,TBD based on problem scoping. The mission emulation capabilities will potentially involve all of the above.,5,TBD during problem scoping.,5,,5,TBD during problem scoping.,Low,High,High,High,TBD during problem scoping. We will consider bringing our vehicles for joint pursuits with Purdue for the DoD.,5,TBD,High,High,High,High,High,High,High,"Ideally we would have heterogenous aavs as a swarm, but again specifics to be discussed during scoping. Digital twins would be used during the development phase and actual aavs for demonstrations.","Yes, I would like to attend in person.",,,
8/31/24 19:15,vgude@elon.edu,Vinayaka,Gude,AAV Operations,1,1,,,,1,,,,1,1,1,,1,,,1,,,,1,,,Feasibility analysis for real-time disaster response planning using swarm UAVs ,"The exploration of Unmanned Aerial Vehicles (UAVs) for a wide range of use cases across various fields in recent years makes it essential to evaluate their operability. Our research focuses on swarm UAV operations aimed to collect data that will inform transportation disruptions caused by debris and flooding, the need for search and rescue operations, and disaster relief resource delivery planning in extreme weather conditions. 
We aim to model and understand how variations in temperature, wind speed, and precipitation affect a swarm of UAVs for different applications primarily within the scope of disaster management using a complex System of Systems framework and scenario-based simulations. The input data for the models will be estimated from the real-time testing in the Purdue Urban Canyon Lab (PUC) and the Digital Twin Engine & Simulator will be used to carry out scenario-based simulations. We aim to explore problems such as sensors becoming less accurate or even failing in harsh weather conditions. The UAVs will be equipped with RGB cameras, thermal cameras, anemometers, and LiDAR along with other required sensors. The data and research services within the AIrTonomy infrastructure will be used to assess the efficiency, disruptions, and latency concerns in data transmission. The results from our study will provide valuable insights into the UAV operability in adverse environments and contribute to the development of resilient UAV systems, capable of reliable operation in a wide range of weather conditions.",,5,"We plan to use this facility to evaluate signal degradation from swarm UAVs for real-time disaster mapping under varying wind, precipitation and temperatures.",1,Not required for our research,5,We plan to use PUC to validate the reliability of AI models for GIS elements detection and further explore the impact of signal degradation on the data collection and AI models under varying weather conditions. ,5,To test and optimize swarm UAV operations for disaster impact assessment.,Low,Low,High,High,"Our research would require UAVs equipped with RGB cameras, thermal cameras, anemometers and LiDAR ",1,To test and optimize swarm UAV operations for disaster impact assessment.,High,High,High,High,High,Low,High,We aim to use the digital twin to simulate extreme weather conditions such as hurricanes and wild fires to estimate the impact on the swarm UAV operations. ,"Yes, I would like to attend in person.",No,,
9/5/24 1:57,branch@qsbg.network,Benjamin,Branch,AAV-enabled real-time sensing,1,1,1,,1,1,1,1,1,1,1,1,1,1,1,,1,1,,,,,1,Edge AI w/ intelligent digital twins & UAVs ,"How can UAV;s and AAV's AI capability be optimized with edge computing with Kove SDM and SNO and 3 worker node installations? How does this transform into workforce development? Which such edge configurations what is the most optimized high fidelity that can be achieved for process simulation in terms of, speed, data rate of predictive maintenance? How can heavy cargo drones safely support the daily distribution of fish on ice?  What computer vision is necessary for food quality catch to food approval to sale and can that be a valid process simulation? How can drones help in counter-intelligence and human trafficking activity with a digital twin? What are the workforce and legal needs to makes sustainable drone operations? How can drones best integrate with a food processing digital twins for critical pathway operation detection in multiple locations? Does Kove Software-defined memory eliminate the edge-digital twin-drone memory bottlenecks? How can edge computing optimize swarm intelligence in an emergency event? Can Emboided AI benefit from multiple workload demands by using Kove software defined memory and OpenShift? How does Red Hat Microshift performs in comparison to hardware in a drone fleet? How can Nb-Iot be integrated with drone operations for location, navigation or emergency response?",https://drive.google.com/open?id=1KghPSzPndYiakh4-vhnAh0YOdK7u21t7,5,"distance over oceans, radio, Nb-Iot",5,We have health care and medical workforce needs that we could use this  to build a workforce outcome.,5,We have rural community drop zones to mimic and figure out for policy development. We have rainy and dry seasons that need simulated along with ocean distance,5,"Yes, we need to practice a drone airline capability  for heavy cargo",High,High,High,High,We need to train on all for monitoring of coast and areas of crime. We are looking into such vehicles.,5,We need to have simulation or use cases that are both urban and rural? We need to support a fishing fleet of boats.,High,High,High,High,High,High,High,"We need to support industrialize food processing outcomes and manage critical supporting operations of seafood processing such as water desalination and animal waste. We need to optimized the  food processing for rainy seasons and dry seasons. Health care services, bot for workers and community. A circular economy is our dream. digital twin We have digital curation for women that critical for workforce development and cultural preservation","No, I am not able to attend in person but I would like to be considered for the best contribution award.","Jarvis Green jarvis@greenreef.org and Kartik Ariyur kartik.ariyur@gmail.com (he may represent us for the meeting, he works at Purdue",I did not see a way to up load my poster,
9/4/24 13:30,bryantbeeler@gmail.com,Bryant,Beeler,AAV-enabled real-time sensing,1,1,,,1,,,,,1,1,,,,,,,,,,,,,"Autonomous Bamboo propagation for accelerated co2 sequestration, biomass, and materials","How can autonomous drones, utilizing the AIrTonomy infrastructure, optimize the propagation of bamboo for biomass generation and CO2 sequestration through remote and continuous operations, and what are the key factors that influence the effectiveness of this approach in large-scale ecological restoration and urban cooling projects?

This question aims to investigate the effectiveness of using AI-driven autonomy to perform continuous, precision-controlled bamboo propagation for environmental sustainability goals. It will focus on how AIrTonomy's remote infrastructure enables complex, long-term operations and what variables affect performance and outcomes.",https://drive.google.com/open?id=1NnPfOD2JDxyZM1qSXy6LaDXiQ-n6zK89,4,The sites for bamboo propagation are often remote and resource constrained. So the remote capbility allows the most qualified individual to perform the operation efficiently. I need to simulate deep forest and remote locations. ,3,I would like to use the SOC to perform human operator integration and refine the autonomous model to emulate the human decisionmaking until the system is fully autnomous. ,5,The forest will be dense and full of obstacles. I need to test a system of systems approach and ensure redunancy so that no loss of capbility effects the entire system and the system can be recovered quicly. ,1,n/a,Low,Low,High,High,mainly Lidar and computer vision onboard processing. Yes I'd like to bring my own vehicles also,2,not very important. ,High,High,High,High,High,High,High,We'd like to use a digital twin of the forest to track and trace the progression end to end. Also the digital twin will inform the most strategic pathway forward and resource management toward optimization ,"No, I am not able to attend in person but I would like to be considered for the best contribution award.",n/a,Im already developing this capability with capstone and i'm a DTECH student. This opportunity would accelerate my work and allow me to develop and field this technology more efficently. ,
9/8/24 21:58,cstewart@cse.ohio-state.edu,Christopher,Stewart,AAV-enabled real-time sensing,1,1,,,1,1,1,,,1,,1,,1,,,,,1,,1,,1,"AFFORDSS: Autonomy For Frequency, Operational, Resolution, and Duration Scaling for Swarms","Research question: Can a community of researchers discover novel drone management techniques that will scale the abilities of AAV swarms in terms of sensing frequency, sensing resolution, sensing operations, and duration of missions without increasing the lifetime cost of AAV swarm deployments?

AirTonomy infrastructure: AirTonomy would provide multiple benchmark scenarios (digital agriculture, traffic surveillance, etc) and real-world flight infrastructure for legal, long-distance, out-of-sight missions.

AI/Models Architectures and Systems: We envision a research community similar in spirit to the ""Top500"" and/or ""Green500.""  This community will develop rules to validate research platforms intended to deploy swarms for long-running land-use surveillance of argiculture, forestry, and urban environments.  The platforms will be ranked along the dimensions in the AFFORDSS acronym (sampling frequency achieved, resolution supported, operations supported, and duration of execution) and cost.  The community would strive to achieve functional swarm deployments without increasing costs.  

Long-term impact: The AFFORDSS Community would attract researchers to this emerging area.  In the long-term, the platforms developed would be integrated into industry products to reduce the cost and improve functionality of AAV systems broadly.",,5,,5,,5,,5,,Low,Low,High,High,Our community will target sUAS.,1,,High,High,High,High,Low,Low,High,,"Yes, I would like to attend in person.",Jayson Boubin (jboubin@binghamton.edu),,
9/8/24 14:46,devesh.upadhyay@saabinc.com,devesh,upadhyay,AAV-enabled real-time sensing,,,,,,,,,,,,,,,,,,,,1,,,,PINNS as a physics preserving learning method for complex dynamical systems,Developing Physics Informed Neural Networks for creating fast surrogate models of dynamical systems.  This will enhance the ability to produce synthetic data at a rate faster than standard simulation while being more physically accurate relative to purely data driven surrogate models and ROMs.  This also gives PINNS better generalizability and explainability relative to purely data driven approaches.  In this talk we discuss the need for data in the development cycle of UAM and how PINNS can help reduce the burden of data needs . ,,5,Flight data under various aero-environment conditions.  ,5,,5,,5,,High,High,High,High,,5,,High,High,High,High,High,High,High,,"Yes, I would like to attend in person.",,,
9/8/24 19:52,jgarriso@purdue.edu,James,Garrison,AAV-enabled real-time sensing,1,1,,,,,,,,,,,,,,,,,,,,,1,Autonomous UAVs for development of (bio-)geophysical model functions supporting Earth remote sensing,"Spaceborne remote sensing can provide global measurements of key environmental variables needed to drive forecast models and support applications such as disaster response, agricultural management, and insurance risk pricing.  All remote sensing modalities require some form of a model function to provide the relationship between observed electromagnetic quantities (e.g. reflectivity or spectral radiance) and bio-geophysical variables (e.g. soil moisture, or vegetation health).  Proper application requires known error bounds and range of applicability. Model functions are usually empirical, developed from extensive field experiments in which the electromagnetic observables are collected co-incident with in situ “truth” data. This is a slow and meticulous process. A model that is statistically significant over the full range of sensitivities often takes years to develop. Field experiments from fixed towers provide long term data records but only for point locations within one ecosystem.  Airborne campaigns are often used to collect remote sensing data over regional areas, but the expense of operating piloted aircraft severely limits their spatiotemporal range. 

This use case incorporates two components of the AIrTonomy program:  Windracer UAV platforms and research in automated path planning and operations.  UAVs can be operated at a substantially lower cost than a piloted aircraft.  Innovative tools could be developed for optimal planning and guidance to conduct precise repetitive coverage over regional test areas outfitted with in situ sensors at minimum total cost.  The scientific contribution of these results would be realized through accelerating the development time and reducing total cost for Earth remote sensing missions.  Signals of opportunity (SoOp) is a new modality, repurposing communication satellite emissions to make measurements in bands inaccessible to conventional microwave remote sensing. With the recent spaceborne test of a prototype instrument on the SNOOPI mission, SoOp would make an ideal candidate for demonstrating this approach. 
",,1,N/A,3,Path planning and operation of UAV remote sensing. ,1,N/A,4,Flight operations of UAV's for Earth remote sensing,High,Low,High,High,Flight or prototype remote sensing instruments for calibration/validation activities and instrument development,3,N/A,Low,High,High,High,High,High,Low,Perhaps in path planning ,"Yes, I would like to attend in person.",N/A,N/A,
9/10/24 2:48,kdantu@buffalo.edu,Karthik,Dantu,AAV-enabled real-time sensing,,1,,,,,,,,1,,,,,,,,,,,,,1,Adaptive Multi-Modal Sensing For Precision Agriculture and Beyond ,"Real applications such as crop monitoring (our NASA AIST project) or identification of archeological areas of interest (our IMPERO project) require coordination between satellite sensing, UAV sensing across multiple modalities (color, thermal, hyperspectral) to identify areas of interest. Design of such applications has many moving parts including adaptive UAV autonomy, sensor characterization, sensor correlation, and gradual integration before full testing. Airtonomy testbed would greatly help systematically study each of these pieces before we put them together for the full deployment.  ",https://drive.google.com/open?id=1riqbZ5NHuW1pWoU6QYUhuJ8uS-yS-cF6,4,Likely all of the above. ,4,"Correlating multiple sensors - thermal, hyperspectral and visual. ",4,Sensor correlation across modalites for the science objective. ,1,,Low,Low,High,High,Likely bring our own but could use your quadrotors if something similar is available. ,1,,High,Low,High,Low,Low,Low,High,Emulation and gradual deployment of various components. ,"Yes, I would like to attend in person.",,,
8/30/24 12:19,mcrawford@purdue.edu,Melba Crawford,Crawford,AAV-enabled real-time sensing,,1,,,,,,,,1,,,,1,,,1,,,,,,,"Determining Soil Moisture, Canopy Water Content, and Plant Biomass from SAR on a WINDRACER Platform ","Problem Statement. Soil moisture and vegetation water content impact plant development, nutrient transport, and biogeochemical cycles, as well as characterizing the condition and biomass of the canopy of row crops and forests. Because it can penetrate the soil surface and provide high-resolution, all-weather, day-and-night data, Synthetic Aperture Radar (SAR) is a valuable technology for measuring these variables. However, the spatial and temporal resolution of existing satellite systems is inadequate for plot and field/forest scale research, crewed aircraft are too costly, and the flight time traditional UAV rotorcraft is too low.  
Proposed method and usage of AIrTonomy. We have successfully developed an S-band SAR system in collaboration with DroneSAR (Pty) Ltd., a spinoff of the U. of Cape Town, SA., that is integrated on a Freefly Alta-X UAV platform. We propose to advance its capability by integrating it with the Windracer platform, acquiring data with improved geometry at higher altitudes over extended areas. We also propose to incorporate the georeferenced SAR data products into our recurrent neural network-based maize yield prediction models and investigate their integration into fire risk models. 
Expected outcome. The project will demonstrate the capability of the SAR system to acquire operational data from the Windracer platform and produce soil moisture and canopy water content data products with approximately .5 m spatial resolution.
Scientific contribution.  The project will advance UAS technology at relevant spatial scales and demonstrate its application in precision agriculture and fire risk modeling. The long-term goal continues to be to provide reliable, high-resolution soil moisture data that can inform decision-making processes and optimize management practices across various application domains. We intend to conduct this interdisciplinary project in partnership with crop scientists and forest modelers at Purdue.
",,2,,2,,3,,5,,High,,,,We want to install a SAR Sensor on the Windracer platform,1,,High,,,High,,High,High,"We may want to simulate the sensor/environment interaction, as well as the SAR sensor.","Yes, I would like to attend in person.",Jinha Jung (jinha@purdue.edu),,
8/9/24 11:22,prithvi.manjunatha.b@gmail.com,Prithvi,Manjunatha,AAV-enabled real-time sensing,1,,,,,,,,,,,,,,,,1,,,1,1,,,Test,"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.",,5,,4,,5,,3,,High,High,Low,High,,4,,4,,,,,,,,"Yes, I would like to attend.",,Test,
9/10/24 11:03,shearer.95@osu.edu,Scott,Shearer,AAV-enabled real-time sensing,,1,,,,,,,,1,,1,,,,,,,1,,1,,,Private Cellular Networks for On-Farm Connectivity and Enhanced Data Transfer Transfer ,"Ohio State University is building wireless/edge computing infrastructure to drive the adoption of AI in Midwestern agriculture. The emergence of AI driven product offerings in agriculture will be limited by an agricultural producer’s ability to collect and transmit data from ground and airborne platforms. Private cellular networks allow end users to prioritize communications between connected platforms, edge computing, and cloud environments. Currently, 24 companies offer AI-driven vision systems for deployment in agriculture. As products offerings and producer adoption expand, the value of these systems will be limited by access to inference models trained for an expanding variety of use cases. The capability to move large amounts of data to processing pipelines, such as ICICLE, will further enhance the capabilities of these systems thereby enabling agricultural producers to fine-tune inferencing to specific crops and specialized cropping practices.",,4,,4,,4,,4,,High,Low,High,Low,,4,,High,High,High,High,Low,High,High,,"Yes, I would like to attend in person.",,,
8/21/24 15:47,vaneet@purdue.edu,Vaneet,Aggarwal,AAV-enabled real-time sensing,1,,,,1,1,,,1,1,,1,,,1,,1,1,1,,1,,,UAV Control with Reinforcement Learning,"1. Research Question:
   - The core research question is: ""How can reinforcement learning (RL) be effectively applied to optimize UAV control and scheduling to achieve efficient route planning and target detection while considering real-time environmental constraints and resource limitations?"" This includes investigating different RL approaches to improve UAV route planning and dynamic scheduling to minimize fuel consumption, maximize threat avoidance, and ensure comprehensive area coverage.

2. Components of the AIrTonomy Infrastructure:
   - The AIrTonomy infrastructure likely offers various tools and platforms that can facilitate reinforcement learning, UAV control, and data analytics. You would want to utilize:
     - Real-time Data Processing Modules: For processing UAV telemetry and environmental data.
     - Simulation Environment: To simulate various UAV missions and train the RL models.
     - AI/ML Model Training Frameworks: For developing and deploying RL algorithms.
     - Autonomous Control Systems: To integrate with RL models for UAV control and decision-making.

3. Design and Validation of AI/ML Models:
   - The RL models will be designed to handle the UAV route planning and scheduling tasks. The AIrTonomy simulation environment will be used to create realistic mission scenarios where the RL models can be trained and validated. The process includes:
     - Model Development: Developing the RL models that can predict optimal actions for UAVs based on current state information (e.g., fuel levels, current position, detected threats).
     - Simulation Testing: Testing these models in a simulated environment provided by AIrTonomy to validate their effectiveness under various conditions.
     - Real-world Validation: Deploying the RL models in real UAV missions using AIrTonomy’s autonomous control systems to assess their performance in real-world settings.

4. New Discoveries and Broader Impacts:
   - This research could lead to new discoveries in how reinforcement learning can be tailored to dynamic, real-time decision-making scenarios in UAV applications. The broader impacts include:
     - Enhanced UAV Mission Efficiency: By improving route planning and scheduling, the missions become more efficient, with lower fuel consumption and better coverage.
     - Improved Threat Detection and Avoidance: Enhanced UAV autonomy will result in more reliable detection and avoidance of threats in complex environments.
     - Generalization to Other Domains: The RL-based approaches could be generalized to other domains such as autonomous driving, logistics, and disaster response, where similar optimization and real-time decision-making are crucial.",https://drive.google.com/open?id=19NNROafJiHu5A_TMUbQteVOcQjfODcXk,4,,4,NA,4,NA,4,NA,Low,Low,Low,Low,NA,2,NA,High,Low,Low,Low,Low,Low,High,We can use that as the simulator for RL research. ,"Yes, I would like to attend in preson.",,,
9/9/24 16:03,xxz1037@case.edu,Xuecen,Zhang,AAV-enabled real-time sensing,1,,,,,,,,,1,,,,,,,,,,,,,,Object Detection on UAV High-resolution Images,"1) How can we design an efficient and accurate object detection model to handle high-resolution images on UAVs while meeting real-time processing requirements and operating reliably on resource-constrained platforms? 2) I would like to leverage AIrTonomy's remote computing infrastructure for large-scale simulation and model training. Specifically, the cloud computing resources, data storage, and processing capabilities, as well as the simulation and testing environments provided by AIrTonomy, will be crucial for model development and validation. 3) Firstly, I will use AIrTonomy’s computing resources for large-scale model training, handling extensive high-resolution image datasets. Secondly, I will utilize AIrTonomy’s simulation environments to test the model under diverse conditions, including varying lighting, object occlusion, and environmental complexity, to ensure robustness. Finally, I will employ AIrTonomy’s remote computing capabilities for performance evaluation and optimization to ensure real-time processing capability and accuracy on actual UAV platforms.4) This research will advance high-resolution image processing technology by developing an efficient object detection model that enhances UAV performance in complex environments. This will enable more intelligent UAV applications, such as traffic monitoring, disaster response, and infrastructure inspection, thereby increasing UAV utility and reliability and having a significant impact on related fields.",,3,I'll probably use PURT to test how the model performs on real data,5,"I would use the SOC to test and enhance high-resolution object detection models for UAVs in simulated real-world environments, such as search and rescue missions and precision agriculture. I require high-resolution cameras, LiDAR, and thermal sensors to test the model's performance in multi-modal and challenging conditions, such as low light or occlusion. The motion capture system could track UAV movements and operator gestures, enabling analysis of human-robot interaction and the accuracy of UAV actions in simulated environments.",1,N/A,2,N/A,High,High,High,High,"I would like to use UAVs equipped with high-resolution cameras, LiDAR, and thermal sensors to test my object detection models in real-world scenarios like search and rescue or precision agriculture. Onboard computing resources capable of handling high-resolution data processing would be essential. I would also study how these vehicles perform in dynamic environments, particularly for beyond visual line of sight (BVLOS) operations. At this time, I do not plan to bring my own vehicles.",1,N/A,High,High,High,High,Low,Low,High,"For my research on high-resolution object detection in UAVs, digital twins could be highly valuable. They would allow me to simulate complex real-world environments and test detection models before deploying them on physical UAVs. This would improve the accuracy and robustness of the models by providing a realistic testing ground. The ability to upload and run simulations, initiate new experiments, and extend digital models tailored to UAV operations would significantly enhance the development process, saving time and resources while optimizing model performance for different scenarios.","Yes, I would like to attend in person.",,,https://drive.google.com/open?id=19r0Od7NqqTcd2yItwVQyFmcsl023l6E4
9/10/24 14:21,zhangyuxuan@ufl.edu,Yuxuan,Zhang,AAV-enabled real-time sensing,1,,,,,,1,,,1,,,,,,,,1,,,,,1,Testing FoveaCam++ for Aerospace Applications,"1) Quantitatively analysis on the amount of advantage does our system have over traditional gimbal cameras; Testing zero-shot based target identification; Testing custom developed streaming codec for this system.
2.) Access to the test field;. A multi-rotor copter which can lift 2KG of payload for more than 30 minutes; High speed link to the air vehicle;
3) The capability of our zero-shot target identification needs real-life validation based on multiple scenes. The vast range of test field provided by AIrTonomy fits our needs perfectly.
4) It will be a fundamental step towards the validation on our system's file-application capabilities.
",https://drive.google.com/open?id=1wHR9QwsDeaEODsbhCcl2osxaIm6E6YD5,,,,,,,,,,,,,,,,,,,,,,,,"No, I am not able to attend in person but I would like to be considered for the best contribution award.",,,https://drive.google.com/open?id=1jnkfghS7ugwlQVVEy3Y6Qop8qAEcUCeD
9/8/24 0:28,abilger@purdue.edu,Aidan,Bilger,"Complex, real-world of AAVs",1,,,,,,,,,,,,,,,,,,,,,,1,Threat and Situational Understanding with Networked Online Machine Intelligence (TSUNOMI),"The overall goal of this project is to develop an explainable machine learning framework with multimodal automatic target recognition and sensor resource management for early warning and situational awareness from surface vessels equipped with an automated verification and validation machine learning pipeline, and seek to formulate effective techniques and algorithms to blend information from multiple sensors, such as cameras and radar, that have been deployed in an area to accurately identify objects that might enter that area. We would like to use the PUP airfield and associated airspace to conduct flight testing and data collection utilizing our ground-based sensors and test aircraft. This airspace is especially important as being able to operate BVR and in various weather conditions is critical to emulating high end adversarial threats that will be encountered on the future battlefield. Since this project is centered around an ML model, data collection and system validation are critical to the accuracy and success of the integrated system. An ML model is only as good as the data fed to it, and a diverse set of data will play a large part in its success. PUP can provide the capabilities for this as we will be able to operate frequently due to its proximity to us, while also allowing us to fly more complex missions when compared to the current flight-testing locations available nearby. Overall, this project is crucial to national security as the modern battlefield has rapidly evolved to include vast amounts of UAS, and automated tracking and prediction is critical to developing a modern and reliable multi-tiered air defense system. Additionally, this system can be integrated into civilian airports to determine if an unknown UAS will pose a threat to manned aviation, which has been an emerging issue with the increase in easy to operate consumer UAS. ",,1,,1,,1,,5,We would like to hangar the test aircraft there when we are conducting flight test operations. The aircraft is already equipped with an RTK rover unit and I have a base station. Runway lighting would be ideal for night ops. ,Low,Low,Low,Low,,1,,Low,Low,Low,Low,Low,Low,Low,,"Yes, I would like to attend in person.",,,https://drive.google.com/open?id=1VSCqkA43HuYAtUd-eh-E9SxHckgLjUIU
9/9/24 16:16,fang375@purdue.edu,Wenzhi ,Fang,"Complex, real-world of AAVs",,,,,,,1,,,,,,,,1,,,,,,,,,Hierarchical Federated Learning with Multi-Timescale Gradient Correction,"While traditional federated learning (FL) typically focuses on a star topology where clients are directly connected to a central server, real-world distributed systems often exhibit hierarchical architectures. Hierarchical FL (HFL) has emerged as a promising solution to bridge this gap, leveraging aggregation points at multiple levels of the system. However, existing algorithms for HFL encounter challenges in dealing with multi-timescale model drift, i.e., model drift occurring across hierarchical levels of data heterogeneity. In this paper, we propose a multi-timescale gradient correction (MTGC) methodology to resolve this issue. Our key idea is to introduce distinct control variables to (i) correct the client gradient towards the group gradient, i.e., to reduce client model drift caused by local updates based on individual datasets, and (ii) correct the group gradient towards the global gradient, i.e., to reduce group model drift caused by FL over clients within the group. We analytically characterize the convergence behavior of MTGC under general non-convex settings, overcoming challenges associated with couplings between correction terms. We show that our convergence bound is immune to the extent of data heterogeneity, confirming the stability of the proposed algorithm against multi-level non-i.i.d. data. Through extensive experiments on various datasets and models, we validate the effectiveness of MTGC in diverse HFL settings.",,3,,,,,,,,,,,,,,,,,,,,,,,,,,
8/31/24 17:19,jayasravanimandapaka@my.unt.edu,Jaya Sravani,Mandapaka,"Complex, real-world of AAVs",,,,1,,1,,,,,1,,,1,,,1,1,,,,,,Collision Avoidance Strategies for Advanced Air Mobility Using UAS-to-UAS Communications,"Advanced Air Mobility (AAM) has introduced a new mode of air transportation that can seamlessly integrate into our daily lives offering services like air taxis and air ambulances. A challenge for AAM is that urban airspace is already congested with commercial air traffic, highlighting the need for an efficient and autonomous airspace management system. Establishing structured air corridors and enabling UAS-to-UAS (U2U) communications are essential steps toward achieving this autonomy. Air corridors are designated airspaces primarily reserved for AAM traffic, which will streamline the movement of Unmanned Aircraft Systems (UAS). Meanwhile, U2U communications enable efficient Collision Avoidance Strategies (CAS), which are critical in such a crowded environment. 

A key aspect of these systems is the development of CAS that rely on advanced communication protocols to monitor traffic patterns and detect potential collisions. This research explores the design and implementation of CAS using U2U communications. Use cases for U2U communications include merging, maintaining minimum separation, information relay, collaborative sensing, and rerouting. All these use cases demand real-time solutions for managing traffic conflicts involving multiple UAS. 

AIrTonomy project could play a significant role in addressing the challenges of AAM and urban airspace management.  The research’s focus on data analysis, pattern recognition, and predictive modeling aligns well with the needs of AAM, potentially enabling real-time traffic analysis, adaptive routing, and collaborative sensing among unmanned aircraft systems. AIrTonomy's interdisciplinary approach could be applied to various aforementioned use cases, all crucial for managing multiple UAS in congested urban airspace. Additionally, the research’s capabilities in simulation and testing could provide valuable insights into the effectiveness of different CAS and U2U communication protocols, ultimately contributing to the development of safer and more efficient autonomous airspace management systems for the future of urban air mobility.

",,5,Helps us in visualizing the autonomy conditions ,3,,3,,,,,,,,,,,,,,,,,,,"Yes, I would like to attend in person.","mathiaskidane@my.unt.edu, kamesh.namuduri@unt.edu",,
9/9/24 12:06,nkong@purdue.edu,Nan ,Kong,"Complex, real-world of AAVs",,,,,,1,,,1,1,1,,,,,,,1,,,1,,,Intelligent dispatch at the 911 and emergency communications center for UAV-integrated EMS,"We will develop an AI-based decision-support tool that can be used by EMS to provide emergency vehicle dispatch and redeployment assistance and facilitate the operations in a drone-road ambulance coordinated logistic system. We will develop robust Markov decision process (MDP) models and reinforcement learning (RL) algorithms, which can efficiently and reliably incorporate uncertainties on drone flight due to weather and signal degradation effects, and human (both 9-1-1 dispatcher and bystander) performance variations in emergency response. We would like to use the Smart Operations Center. We consider leveraging it to calibrate and validate a digital twin for the UAV-integrated EMS system. New discovery: development of novel robust RL algorithms that work well with appropriate uncertainty modeling of the key components along the EMS delivery process; broader impacts: help inform the infrastructure and policy developments when introducing a new transportation means to an emergency response logistic system.",,2,n/a,3,"perhaps in the future, we would like to use real-time sensing of human cognition. ",5,the infrastructure that can mimic real-world emergency response logistics. ,1,,Low,Low,Low,Low,,1,,High,High,Low,High,High,High,High,,"Yes, I would like to attend in person.",,,
9/8/24 15:07,richardcdeng27@gmail.com,Chuhao,Deng,"Complex, real-world of AAVs",,,,,,1,,,,1,,,,1,,,,,,,,,,Human Sensing: Real-time cognitive load and situation awareness estimation,"Cognitive modeling is a critical part for realizing AIDA3’s vision. In general, there are five research questions related to this topic: 1) how to define and quantify cognitive load (CL) and situation awareness (SA); 2) how to classify and predict CL and SA in real time; 3) can we quantify operators’ expertise using CL and SA during a mission; 4) what is the minimal set up for answering research question 2) and 3); 5) how to design the system so that CL and SA never exceed the safety thresholds. To asnwer those research questions, We propose a multimodal deep learning (DL) approach to estimate CL and SA. To collect the data, we design human subject experiments in realistic settings. In total, 23 human subjects performed multiple tasks with varying complexity in their role as remote operators of unmanned aerial vehicles. The collected data subsumes around 26 hours and more than 1 TB of multimodal data. Our results and analysis show the potential of our dataset for future
research and real-world applications related to brain-computer interfaces, assisted technologies, or other forms of real-time behavioral intervention. Currently, our human sensing research is still at the experiment stage and has not been deployed in any real-life operations. In the future, we want to be able to 1) collect CL and SA of a team of UAV operators who do not just simply conduct missions in experimental tasks, but also in real tasks, 2) design CL and SA-based feedback system, which triggers physical features (e.g. vibration and alert sound) and software features (e.g. pop-up windows and blinking icons), and 3) test our model in realistic UAV operations. SOC provides the essential infrastructure and equipment for our future goals mentioned above. For instance, SOC has access to the most advanced physiological sensors, which helps us to achieve goal 1. In addition, SOC is designed to be able to conduct real UAV operations remotely, which provides us the necessary hardware and software to achieve goal 2 and 3. We believe that by using SOC, our
human sensing research will advance significantly and we can bring our research
products to real life.",,5,,5,,5,,5,,,,,,,,,,,,,,,,,,,,https://drive.google.com/open?id=1HCw4eWaLsExwFSNTytCByiwzfHkF2ASU
9/9/24 0:29,wsribunm@purdue.edu,Worawis (Willis),Sribunma,"Complex, real-world of AAVs",,,,,1,,,,,,1,,,,,,,,,,,1,,Robustness and Safe Verification of Total Energy Control System Under Disturbances,"1) How can we verify the safety for a fixed-wing vehicle during autonomous landing operations under external disturbances? 2) PURT, PUP, and SOC testing facility 3) Experimental testing to validate the safety verification model. PURT's motion capture system provides a ground truth in an indoor environment. This allows a safe test environment for preliminary development of the research prior to outdoor or real-world testing. Once a level of confidence is achieved, outdoor testing on larger vehicle will be crucial for the deployment of future autopilot module. SOC will allow a leading-edge operation center for realistic environmental condition for remote operators. 4) This research will further ensure the safety operation of UAVs when the weather and disturbances can be anticipated.
",,5,,4,,4,,5,,High,Low,High,High,,4,,,,,,,,,,"Yes, I would like to attend in person.",,,https://drive.google.com/open?id=1PpdQ9zikwaimhpxHqlXA0UndQDadh0bk
9/6/24 14:07,ehsanesf@buffalo.edu,Ehsan,Esfahani,Human-autonomy-teaming (human-in-the-loop),,,,,,1,,,1,,,,,1,,,,,,,1,,,HOW REAL-TIME HUMAN SENSING WILL FACILITATE RESEARCH ON SAFE HUMAN-AUTONOMY SWARMS,"Advanced Cyber-Physical Systems rely on human-AI teaming in which understanding the interaction dynamics between human cognition and AI agents is key to enhancing efficiency. This requires thoughtful design of feedback mechanisms, reasoning processes, and compliance protocols that account for their impact on human cognition with the. The goal is to foster shared awareness between humans and AI while calibrating feedback to keep operators informed without overwhelming them with excessive information. Current research in explainable AI highlights that human acceptance of AI suggestions is influenced by factors such as cognitive biases, trust imbalances, and the perceived cost-benefit balance in the task at hand. Explainable AI is thought to gain broader acceptance if it is easy to understand. However, much of this research focuses on interactions between a human operator and a single AI agent, often in unrealistic settings with limited real-world consequences for failure.
This case study explores human operators interacting with multiple Unmanned Aerial Vehicles (UAVs) to monitor an unknown environment. Due to differing environmental perceptions, human commands may be seen as infeasible or risky by AI, potentially leading to AI non-compliance. We aim to examine how non-compliance rates and feedback mechanisms affect human cognition, trust, and the role of individual differences in this interaction.
Building on our previous studies, we propose using the Smart Operation Centre (SPC) to collect physiological data (EEG, fNIRS and eye-tracking) from human operators working with multiple UAVs. We will estimate mental engagement, workload, distraction, and attention from these modalities to better understand AI-human dynamics. Our goal is to develop explainable AI systems that enable effective and transparent dialogue between humans and AI, fostering mutual awareness and trust, thereby enhancing decision-making and human reliance on AI. This talk will review our prior works and outline plans for utilizing the SPC to achieve these objectives.",,2,,5,Human subject study for recording neurofeedback of operators interacting with multiple agents ,2,I envision utilizing the facility for real human-uav interactions instead of studying the interaction in the simulation environments.,2,NA,Low,Low,Low,Low,NA,1,NA,High,High,Low,High,High,Low,High,"For developing enhanced simulation environment were different design of feedback mechanisms, reasoning processes, and compliance protocols of the explainable AI can be tested and validated.","Yes, I would like to attend in person.",,,
8/31/24 17:27,hcandido@unicamp.br,Henrique,Oliveira,Human-autonomy-teaming (human-in-the-loop),1,,,,1,1,,,1,,1,,,,,,,,,,,,,UAV real-time 3D path planning for Transport applications based on safety and onboard processing,"The autonomous flight capability of UAVs raises safety concerns, especially regarding people near the operations. In this context, this research evaluates whether AI/ML integrated with UAVs ensures safe and efficient real-time path planning for drone logistics operations. In the proposed method, an initial route for the UAV is defined through offline path planning. Until the destination is reached, real-time images are captured to detect people along the route. The three-dimensional coordinates of the person closest to the UAV are determined, and new flight restriction areas are established. Finally, real-time 3D path replanning is carried out through online path planning. The AIrTonomy Infrastructure components can be crucial for this research, including conducting experiments in an indoor environment (Motion-Capture Indoor Lab), human detection in UAV images using the AI/ML workspace and Digital Twin Engine & Simulators, Communication & Networking Infrastructure for connecting the Virtual Operations Portal with APIs and Fleet & Onboard Sensing, and the development of a portability algorithm (Onboard Software & Toolkits) for conducting experiments at the Urban Canyon Lab (PUC). The expected results include the development of algorithms and software for pre-path planning (offline), real-time detection of people in UAV images, and adaptation of UAV paths in real-time via onboard processing (online path planning). Incorporating people as a factor in UAV path planning and establishing a method capable of real-time path replanning with onboard processing are the main scientific contributions of this work.",https://drive.google.com/open?id=1xQcnscJzU-id4AyL2Pe2miCtehhT_FWM,5,It can be used for to validating the offline routing method,4,It can be used to check the results of our online approach,5,Conducting experiments at PUC to validate the offline routing method,2,,,,High,High,We eould like to experiment different platforms and cameras to performe the real-time people identification and UAV motion.,1,,High,High,High,High,,,High,We believe that performing experiments in simulated environment is an important step for validating our methos.,"Yes, I would like to attend in person.",,,
8/11/24 16:42,jshreeku@purdue.edu,Jayanth,Shreekumar,Human-autonomy-teaming (human-in-the-loop),1,,,,,,,,1,1,,,,,,,,,,1,,,,"""Lorem ipsum dolor sit amet","Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.
Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt ut labore et dolore magnam aliquam quaerat voluptatem. Ut enim ad minima veniam, quis nostrum exercitationem ullam corporis suscipit laboriosam, nisi ut aliquid ex ea commodi consequatur? Quis autem vel eum iure reprehenderit qui in ea voluptate velit esse quam nihil molestiae consequatur, vel illum qui dolorem eum fugiat quo voluptas nulla pariatur?",,3,,5,,3,,2,,Low,High,High,,,1,,3,,,,,,,,"Yes, I would like to attend.",,,
9/9/24 16:18,sbyeon@purdue.edu,Sooyung,Byeon,Human-autonomy-teaming (human-in-the-loop),,,,,,1,,,,,,,,,,,,,,,1,,,Optimal Function and Attention Allocation for Human-Autonomy Teaming,"Research Question: How can computational cognition models optimize the allocation of functions and attention in human-autonomy teaming to enhance performance and maintain situational awareness while minimizing human cognitive workload?

AIrTonomy Components: I would utilize the AIrTonomy infrastructure's test facility to validate adaptive function allocation policies using hardware systems. Specifically, the infrastructure’s ability to demonstrate human-AI interactions in complex environments and measure task performance in dynamic, real-time scenarios would be essential.

Design and Validation: Using AIrTonomy, I would simulate disaster relief scenarios, leveraging the infrastructure’s ability to test varying function and attention allocation policies in real-time human-autonomy teaming settings. Cognitive models would be integrated to evaluate human workload and situation awareness, and the AI/ML components would optimize these allocations. The simulation outputs would be validated using a digital twin of the team dynamics, refining the models iteratively.

Broader Impact: This research could lead to more efficient and adaptive AI systems that account for human cognitive limitations, improving teamwork in critical applications such as disaster relief and robotics. The findings could influence future AI systems in multiple domains, enhancing both safety and operational efficiency.
",,5,"distance over oceans, radio, Nb-Iot",5,We have health care and medical workforce needs that we could use this  to build a workforce outcome.,5,We have rural community drop zones to mimic and figure out for policy development. We have rainy and dry seasons that need simulated along with ocean distance,5,"Yes, we need to practice a drone airline capability  for heavy cargo",High,High,High,High,We need to train on all for monitoring of coast and areas of crime. We are looking into such vehicles.,5,We need to have simulation or use cases that are both urban and rural? We need to support a fishing fleet of boats.,High,High,High,High,High,High,High,"We need to support industrialize food processing outcomes and manage critical supporting operations of seafood processing such as water desalination and animal waste. We need to optimized the  food processing for rainy seasons and dry seasons. Health care services, bot for workers and community. A circular economy is our dream. digital twin We have digital curation for women that critical for workforce development and cultural preservation","Yes, I would like to attend in person.",,I cannot attend the entire time but will attend in person as much as possible.,https://drive.google.com/open?id=1StJ9JSKHUSNXMP-wbeCTpL2NvgAzadtK
8/23/24 10:41,soguchie@purdue.edu,Somtochukwu,Oguchienti,Human-autonomy-teaming (human-in-the-loop),1,,,,1,1,,,,,,,,,,,,,1,,1,,,Human-aware Learning-based techniques for autonomous aerial vehicle operations,"1) Research question?
How do we leverage feedback from humans in the form of preferences or rankings of expert demonstrations to teach intelligent Autonomous Aerial Vehicles (AAVs) how to safely perform firefighting and critical search and rescue missions. 

2) Components I would like to use?
Given the interoperability of flight missions, these components may not be used in isolation. Hence, I would like to use them in a collaborative manner such that we can reap the benefits of both the indoor and outdoor facilities. 

Firstly, the Indoor Motion Capture Lab (PURT) serves as a significant component to capture data about the AAV's position and altitude using motion cameras, LED markers, and sensors. These sensors are relevant to capture in real-time the vehicle's behavior during operation time, and through these data, we can make adjustments to parameters that affect the vehicle on flight, including the speed, altitude, and direction.
Secondly, the Smart Operations Center (SOC) will serve as an interactive and immersive component for remote operation and testing of the AAVs. From the comfort of this center, flight missions can be planned remotely before the actual operation, including planning for missions for multiple AAVs and swarm operations. This function is very necessary, especially for vehicles on search and rescue missions, as they usually cooperate during such events and can speedily communicate findings with the operators. With the help of multiple screens, remote operators can multitask and effectively plan missions even beyond visual line of sight (BVLOS).

Next, the Purdue Urban Canyon Lab (PUC) is an important facility for observing the simulations and remote operations in real life. For firefighting and search and rescue missions, this is a necessary outdoor facility that can help deploy these AAVs to monitor how they can navigate uncertain environments like forest zones, urban canyons, and invasive buildings.

Lastly, these AAVs can also be used for payload and cargo delivery for providing survivors of a fire incident with supplies, medicines, or communication devices. Here, the Purdue Unmanned aerial Proving Ground (PUP) Airfield can serve as a dedicated runway for testing takeoff and landing of these vehicles. Heavier payloads require larger UAVs, and the PUP Airfield can be a useful testing ground for collecting ground truth data in real time.


3) How I would use these components and specific requirements?
Given my proposed use case of teaching AAVs how to perform critical operations like firefighting and search-and-rescue missions, we would require a lot of data, both synthetic and real, on how to perform these operations. We would also require vehicle data from similar operations to teach these AAVs the best strategies for successful missions. Firstly, we need to gather data about how the vehicles operate in simulation. For this, the PURT will be relevant for capturing such information. Through the use of sensors installed on these vehicles, we can gather data related to their flight distance, velocity, position, and altitude. Specifically, we may require vision and motion cameras, radar sensors, LiDAR sensors, and ultrasonic sensors. For the vehicle's relative and absolute positions for swarm operations, we may also require GPS tracking systems and odometry sensors. With these data, we can remotely track vehicular information necessary for flight. 
For safety purposes, we can also incorporate mixed reality testing and sensor simulations to virtually place sensors on AAVs to understand how these sensors gather and track vehicle information. This method of simulation can also help us to know where to correctly place these sensors on the vehicle board to gather as much accurate information as possible in real-time.

Moreover, vehicle-to-vehicle communication and operator-to-vehicle communication are very important during critical missions such as search and rescue. With the help of the sensor simulations, we can observe swarm operations, especially in GPS-denied environments or areas with GPS signal degradation. This is very necessary to ensure that the remote operator is aware of the current state of the vehicles at any point in time during operation, especially beyond visual line of sight.

To solve this problem as specified in my use case, I plan to leverage AI/ML technologies. Specifically, to teach these vehicles how to perform safely during field operations, we may require inverse reinforcement learning (IRL), an ML technique that uses demonstration data from humans or other vehicles. Given the criticality of firefighting operations and search-and-rescue mission operations, we need to use data from human experts or demonstration data from successful mission trips to train these AAVs. This will ensure that the AAVs are taking the right actions at any given state during the operation. Given the unavailability of expert demonstration data, we can generally use data from any human expert but incorporate their suboptimality or expertise levels into the training algorithm to ensure that we sample data from experts that have more competence in the task we care about.

Also, with the cost associated with gathering these demonstration data and the difficulty in specifying a reward function that defines the goal of the task for the AAVs, we can adopt the preference-based reinforcement learning (PbRL) technique, which is basically training these vehicles from human feedback. Here, for every given state of the AAV and action it takes, a human can return feedback in the form of a score. This scoring-based approach can ensure that we train the AAVs with the highest-scoring action for any particular state. In the event of any difficulty in giving such scores, we may adopt a ranking-based system. In this approach, the human expert can rank vehicle trajectories based on how optimal the actions are at any given state.

Given the indoor facilities provided by the PURT and the SOC, we can train these AAVs in simulation rather than in a real environment. Besides teaching the autonomous agents how to navigate such safety-critical operations, this IRL or PbRL approach can be useful for teaching the AAVs how to successfully take off and land. For this, we can use the PUP Airfield to validate the performance of the AAVs after training is completed. 


4) Expected discoveries and real-world impact?
Given my proposed use case and the adoption of RL techniques alongside traditional control methods, we expect to see these autonomous vehicles make human-aware decisions with improved performance, especially in uncertain environments and safety-critical missions. By incorporating a human-in-the-loop approach, we can integrate humans and machines for effective communication and collaboration of multilayered tasks like AAV flight operations. Hence, by leveraging this approach, we aim to get a step closer to our goal: having a single human control tens to hundreds of AAVs while achieving huge success in delivery and/or search-and-rescue missions.",,4,,5,,4,,4,,High,High,High,High,,,,,,,,,,,,"No, I am not able to attend in person but I would like to be considered for the best contribution award.",,,
9/9/24 16:10,yih@purdue.edu,Yuehwern,Yih,Human-autonomy-teaming (human-in-the-loop),,,,,,1,,,,,1,,,,,,,,1,,1,,1,Operation planning for AAVs in humanitarian and medical supply chains,(1) How to plan and make decisions on routing and dispatching in emergency response? How does human perform in AAV related tasks under high stress environments?  How do we incorporate those considerations in the design of AVV operations to support logistics and supply chains to reach underserved populations and areas needing humanitarian support? (2) most of the cyber components and PURT (3) the sensing information onboard and operational data will inform the simulation model to develop routing and dispatching strategies and provide the testbed for understand human performance in AVV related tasks to mitigate risks for safe and cost effective operations (4) this research will develop the base knowledge to inform how to integrate AVVs in logistics and supply chains to provide speedy and cost effective transport to address the current challenges in emergency response and underserved areas.,https://drive.google.com/open?id=1ouCK0pBH7ihMxRkY38ikkFTpYUXZkXpA,4,"The data collected to correlate physical positions, other sensing data, and human control data will be useful to develop the simulation model for decision support",5,"This Center provides the capabilities to capture human interactions with devices, information/data, and alert system under different operating environments.  This will be necessary to study the human-autonomy performance and the design features needed for safe and effective operations using AAVs in challenging conditions.",4,"It will be important to develop the capabilities and understand the limitations on GPS, RADAR, cameras and communication networks during flights.  I am specifically interested in the feasibility to detect changes to support emergency responses.",3,interested in the data collected during take off and landing for simulation models.,High,High,Low,High,"I won't be using any of the AAVs directly, but interested in data collected during testing.",2,"I won't be using Purdue XTM, but the specifications and performance of the communication network will be of interest in development of routing and real-time response.",High,High,High,High,High,High,High,"Digital twin will be linked to simulation model for operational planning, routing, and real-time control for risk mitigation and cost effective operations","Yes, I would like to attend in person.","Biller, Stephan Robert <sbiller@purdue.edu> ,Salama, Mohamed <salamam@purdue.edu>, Cai, Hua <huacai@purdue.edu>, Khir, Reem <rkhir@purdue.edu>, Lee, Seokcheon <lee46@purdue.edu>, She, Yu <shey@purdue.edu>, Wu, Wenzhuo <wu966@purdue.edu>, Duffy, Vincent G <duffy@purdue.edu>, Lehto, Mark R <lehto@purdue.edu>, Caldwell, Barrett S. <bcaldwel@purdue.edu>",,
9/5/24 0:11,ziran@purdue.edu,Ziran,Wang,Human-autonomy-teaming (human-in-the-loop),,,,,,1,,,1,1,1,1,1,,1,,,1,1,,,,,What We Can Learn from Human-Autonomy Teaming and Digital Twin in Ground Mobility,"1) Research Question:

The key research question would be: How can human-autonomy teaming in ground mobility systems enhance decision-making, operational efficiency, and safety in dynamic and complex environments? Specifically, the study would focus on identifying the optimal balance of human oversight and AI-driven autonomy in ground mobility, such as autonomous vehicles, and how these systems can improve responses to real-time environmental changes, emergencies, or challenging terrains.

2) AIrTonomy Components:

From the AIrTonomy infrastructure, I would like to use the following components:

 • AI/ML Model Training and Deployment: This would include supervised and reinforcement learning modules for training AI agents in various mobility scenarios.
 • Autonomous System Simulation Environments: This provides a virtual platform for testing ground mobility systems in simulated urban and off-road environments.
 • Human-Machine Interface (HMI): This will help study interactions between human operators and AI systems to evaluate response times, decision-making quality, and overall situational awareness.
 • Data Analytics Tools: To capture performance metrics, system failures, and human feedback during simulated missions.

3) Designing and Validating AI/ML Models:

 • Model Training and Testing: Train the AI on various datasets related to ground mobility tasks, such as obstacle detection, path planning, and emergency responses. Use supervised learning for structured tasks and reinforcement learning for more complex, dynamic decision-making situations.
 • Simulations for Testing: Utilize AIrTonomy’s simulation environments to test how well the AI can handle different mobility scenarios, including complex urban landscapes, rural areas, and emergency situations (e.g., weather disruptions).
 • Human-in-the-loop Testing: Incorporate human supervisors in the testing phase to evaluate how humans intervene during AI failures and identify ways to improve collaboration between human operators and autonomous systems.
 • Real-World Trials: Once the models are validated in simulations, implement controlled real-world experiments to further refine and validate the AI models.

4) Broader Impacts and Discoveries:

 • Advances in Human-Autonomy Collaboration: It could lead to the development of new frameworks for effective human-autonomy teaming, which is essential in many industries, including defense, logistics, and public transportation.
 • Improved Safety and Efficiency in Ground Mobility: The research could result in AI systems that handle real-time changes more effectively, improving safety in autonomous ground vehicles, reducing accidents, and increasing operational efficiency in various applications such as military missions or urban transportation.
 • Contribution to AI Ethics and Regulation: By studying the boundary between human and machine decision-making, this research could offer insights into ethical considerations, such as determining when human intervention should override AI in critical situations.
 • Broader Industry Impact: Beyond autonomous vehicles, the findings could apply to other areas such as robotics, healthcare, and even space exploration, where human-autonomy teaming is critical for handling high-stakes, complex environments.",,3,,3,,3,,3,,Low,High,High,High,,3,,High,Low,Low,Low,High,Low,High,,"Yes, I would like to attend in person.",,,